{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1: Getting to know the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/06 17:55:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession, functions, types, Row\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "## DataFrames\n",
    "spark = SparkSession.builder.appName('Canadian wind').getOrCreate()\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "assert spark.version >= '3.0'  # make sure we have Spark 3.0+\n",
    "## RDDs\n",
    "sc = spark.sparkContext\n",
    "assert sc.version >= '3.0'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# General variables\n",
    "data_path = \"data.nosync\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metadata\n",
    "### Stations\n",
    "See [documentation](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt)\n",
    "IV. FORMAT OF \"ghcnd-stations.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    id = '(\\S+)'\n",
    "    latitude = '([-+]?(?:\\d*\\.\\d+|\\d+))'\n",
    "    longitude = '([-+]?(?:\\d*\\.\\d+|\\d+))'\n",
    "    elevation = '([-+]?(?:\\d*\\.\\d+|\\d+))'\n",
    "    state = '([-a-zA-Z0-9_][-a-zA-Z0-9_])'\n",
    "    name = '((\\S+\\s)+)'\n",
    "    delimiter = '\\s+'\n",
    "    any = \".*\"\n",
    "    line_re = re.compile(r'^'+ id + delimiter + latitude + delimiter + longitude + delimiter + elevation + delimiter + state + delimiter + name + any + '$')\n",
    "    splitted_line = re.match(line_re, line)\n",
    "    return Row(splitted_line.group(1), float(splitted_line.group(2)), float(splitted_line.group(3)), float(splitted_line.group(4)), splitted_line.group(5), splitted_line.group(6))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "def stations_schema():\n",
    "    return types.StructType([\n",
    "        types.StructField(\"id\", types.StringType()),\n",
    "        types.StructField(\"latitude\", types.FloatType()),\n",
    "        types.StructField(\"longitude\", types.FloatType()),\n",
    "        types.StructField(\"elevation\", types.FloatType()),\n",
    "        types.StructField(\"state\", types.StringType()),\n",
    "        types.StructField(\"name\", types.StringType()),\n",
    "        #types.StructField(\"gsn_flag\", types.StringType()),\n",
    "        #types.StructField(\"crn_flag\", types.StringType()),\n",
    "        #types.StructField(\"wmo_id\", types.StringType()),\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---------+---------+-----+--------------------+\n",
      "|         id|latitude|longitude|elevation|state|                name|\n",
      "+-----------+--------+---------+---------+-----+--------------------+\n",
      "|CA001010066| 48.8667|-123.2833|      4.0|   BC|        ACTIVE PASS |\n",
      "|CA001010235|    48.4|-123.4833|     17.0|   BC|        ALBERT HEAD |\n",
      "|CA001010595| 48.5833|-123.5167|     85.0|   BC|BAMBERTON OCEAN C...|\n",
      "|CA001010720|    48.5|   -124.0|    351.0|   BC|         BEAR CREEK |\n",
      "|CA001010774|    48.5|  -123.35|     61.0|   BC|        BEAVER LAKE |\n",
      "|CA001010780| 48.3333|-123.6333|     12.0|   BC|         BECHER BAY |\n",
      "|CA001010960|    48.6|-123.4667|     38.0|   BC|    BRENTWOOD BAY 2 |\n",
      "|CA001010961| 48.5667|  -123.45|     31.0|   BC|BRENTWOOD CLARKE ...|\n",
      "|CA001010965| 48.5667|-123.4333|     91.0|   BC|BRENTWOOD W SAANI...|\n",
      "|CA001011467| 48.5833|-123.4167|     53.0|   BC|CENTRAL SAANICH V...|\n",
      "|CA0010114F6| 48.5667|   -123.4|     38.0|   BC|CENTRAL SAANICH I...|\n",
      "|CA0010114FF|   48.55|   -123.4|     88.0|   BC|CENTRAL SAANICH T...|\n",
      "|CA001011500| 48.9333|  -123.75|     75.0|   BC|          CHEMAINUS |\n",
      "|CA001011743| 48.6833|   -123.6|     99.0|   BC|        COBBLE HILL |\n",
      "|CA001011745|   48.65|-123.5667|     61.0|   BC|COBBLE HILL DELOU...|\n",
      "|CA001011775|   48.65|   -123.4|     37.0|   BC|           COLE BAY |\n",
      "|CA001011810| 48.4167|-123.4833|     76.0|   BC|  COLWOOD HATLEY DR |\n",
      "|CA001011860| 48.8333|-123.8333|    177.0|   BC|      COPPER CANYON |\n",
      "|CA001011920| 48.5333|-123.3667|     37.0|   BC|        CORDOVA BAY |\n",
      "|CA001011922| 48.5167|-123.3667|     26.0|   BC|  CORDOVA BAY SOUTH |\n",
      "+-----------+--------+---------+---------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations_input = sc.textFile(data_path + \"/ghcnd-stations.txt\")\n",
    "formatted_lines = stations_input.filter(lambda line: line.startswith(\"CA\")).map(parse_line)\n",
    "cleaned_stations = spark.createDataFrame(data=formatted_lines, schema = stations_schema())\n",
    "cleaned_stations.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "cleaned_stations.write.parquet(data_path + \"/ghcnd-stations-cleaned\", mode=\"overwrite\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Countries\n",
    "See [documentation](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt)\n",
    "V. FORMAT OF \"ghcnd-countries.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    code = '([A-Z][A-Z])\\s'\n",
    "    name = '(([,A-Za-z\\[\\]\\(\\)]+\\s*)+)'\n",
    "    line_re = re.compile(r'^'+ code + name + '\\s*$')\n",
    "    splitted_line = re.match(line_re, line)\n",
    "    return Row(splitted_line.group(1), splitted_line.group(2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [],
   "source": [
    "def countries_schema():\n",
    "    return types.StructType([\n",
    "        types.StructField(\"code\", types.StringType()),\n",
    "        types.StructField(\"name\", types.StringType()),\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|code|                name|\n",
      "+----+--------------------+\n",
      "|  AC|Antigua and Barbuda |\n",
      "|  AE|United Arab Emira...|\n",
      "|  AF|         Afghanistan|\n",
      "|  AG|            Algeria |\n",
      "|  AJ|         Azerbaijan |\n",
      "|  AL|             Albania|\n",
      "|  AM|            Armenia |\n",
      "|  AO|             Angola |\n",
      "|  AQ|American Samoa [U...|\n",
      "|  AR|          Argentina |\n",
      "|  AS|          Australia |\n",
      "|  AU|            Austria |\n",
      "|  AY|         Antarctica |\n",
      "|  BA|            Bahrain |\n",
      "|  BB|           Barbados |\n",
      "|  BC|           Botswana |\n",
      "|  BD|Bermuda [United K...|\n",
      "|  BE|            Belgium |\n",
      "|  BF|       Bahamas, The |\n",
      "|  BG|          Bangladesh|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_input = sc.textFile(data_path + \"/ghcnd-countries.txt\")\n",
    "formatted_lines = countries_input.map(parse_line)\n",
    "cleaned_countries = spark.createDataFrame(data=formatted_lines, schema = countries_schema())\n",
    "cleaned_countries.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### States\n",
    "See [documentation](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt)\n",
    "VI. FORMAT OF \"ghcnd-states.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    code = '([A-Z][A-Z])\\s'\n",
    "    name = '(([A-Z]+\\s*)+)'\n",
    "    line_re = re.compile(r'^'+ code + name + '\\s*$')\n",
    "    splitted_line = re.match(line_re, line)\n",
    "    return Row(splitted_line.group(1), splitted_line.group(2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "def states_schema():\n",
    "    return types.StructType([\n",
    "        types.StructField(\"code\", types.StringType()),\n",
    "        types.StructField(\"name\", types.StringType()),\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|code|                name|\n",
      "+----+--------------------+\n",
      "|  AB|             ALBERTA|\n",
      "|  AK|              ALASKA|\n",
      "|  AL|             ALABAMA|\n",
      "|  AR|            ARKANSAS|\n",
      "|  AS|      AMERICAN SAMOA|\n",
      "|  AZ|             ARIZONA|\n",
      "|  BC|    BRITISH COLUMBIA|\n",
      "|  CA|          CALIFORNIA|\n",
      "|  CO|            COLORADO|\n",
      "|  CT|         CONNECTICUT|\n",
      "|  DC|DISTRICT OF COLUMBIA|\n",
      "|  DE|            DELAWARE|\n",
      "|  FL|             FLORIDA|\n",
      "|  FM|          MICRONESIA|\n",
      "|  GA|             GEORGIA|\n",
      "|  GU|                GUAM|\n",
      "|  HI|              HAWAII|\n",
      "|  IA|                IOWA|\n",
      "|  ID|               IDAHO|\n",
      "|  IL|            ILLINOIS|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "states_input = sc.textFile(data_path + \"/ghcnd-states.txt\")\n",
    "formatted_lines = states_input.map(parse_line)\n",
    "cleaned_states = spark.createDataFrame(data=formatted_lines, schema = states_schema())\n",
    "cleaned_states.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inventory\n",
    "See [documentation](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt)\n",
    "VII. FORMAT OF \"ghcnd-inventory.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "def toDF(data):\n",
    "    splitted_data = data.split(\" \")\n",
    "    while \"\" in splitted_data:\n",
    "        splitted_data.remove(\"\")\n",
    "\n",
    "    return Row(splitted_data[0], float(splitted_data[1]), float(splitted_data[2]), splitted_data[3], int(splitted_data[4]), int(splitted_data[5]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def inventory_schema():\n",
    "    return types.StructType([\n",
    "        types.StructField(\"id\", types.StringType()),\n",
    "        types.StructField(\"latitude\", types.FloatType()),\n",
    "        types.StructField(\"longitude\", types.FloatType()),\n",
    "        types.StructField(\"element\", types.StringType()),\n",
    "        types.StructField(\"first_year\", types.IntegerType()),\n",
    "        types.StructField(\"last_year\", types.IntegerType()),\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/06 15:44:45 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 20 (TID 36): Attempting to kill Python Worker\n",
      "+-----------+--------+---------+-------+----------+---------+\n",
      "|         id|latitude|longitude|element|first_year|last_year|\n",
      "+-----------+--------+---------+-------+----------+---------+\n",
      "|CA001010066| 48.8667|-123.2833|   PRCP|      1984|     1996|\n",
      "|CA001010066| 48.8667|-123.2833|   SNOW|      1984|     1996|\n",
      "|CA001010066| 48.8667|-123.2833|   SNWD|      1984|     1996|\n",
      "|CA001010066| 48.8667|-123.2833|   MDPR|      1984|     1996|\n",
      "|CA001010066| 48.8667|-123.2833|   MDSF|      1984|     1990|\n",
      "|CA001010235|    48.4|-123.4833|   TMAX|      1976|     1978|\n",
      "|CA001010235|    48.4|-123.4833|   TMIN|      1976|     1978|\n",
      "|CA001010235|    48.4|-123.4833|   PRCP|      1971|     1995|\n",
      "|CA001010235|    48.4|-123.4833|   SNOW|      1971|     1995|\n",
      "|CA001010235|    48.4|-123.4833|   SNWD|      1991|     1995|\n",
      "|CA001010235|    48.4|-123.4833|   MDPR|      1971|     1995|\n",
      "|CA001010595| 48.5833|-123.5167|   PRCP|      1961|     1980|\n",
      "|CA001010595| 48.5833|-123.5167|   SNOW|      1961|     1980|\n",
      "|CA001010595| 48.5833|-123.5167|   SNWD|      1980|     1980|\n",
      "|CA001010595| 48.5833|-123.5167|   MDPR|      1962|     1980|\n",
      "|CA001010595| 48.5833|-123.5167|   MDSF|      1962|     1969|\n",
      "|CA001010720|    48.5|   -124.0|   TMAX|      1957|     1971|\n",
      "|CA001010720|    48.5|   -124.0|   TMIN|      1957|     1971|\n",
      "|CA001010720|    48.5|   -124.0|   PRCP|      1910|     1971|\n",
      "|CA001010720|    48.5|   -124.0|   SNOW|      1910|     1971|\n",
      "+-----------+--------+---------+-------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "inventory_input = sc.textFile(data_path + \"/ghcnd-inventory.txt\")\n",
    "formatted_lines = inventory_input.filter(lambda line: line.startswith(\"CA\")).map(toDF)\n",
    "cleaned_inventory = spark.createDataFrame(data=formatted_lines, schema = inventory_schema())\n",
    "cleaned_inventory.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleaned_inventory.write.parquet(data_path + \"/ghcnd-inventory-cleaned\", mode=\"overwrite\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "### Daily summaries latest\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----------+---------+--------------------+-----+---------------+-----+---------------+-----+---------------+----+---------------+----+---------------+----+---------------+\n",
      "|    STATION|      DATE| LATITUDE|  LONGITUDE|ELEVATION|                NAME| PRCP|PRCP_ATTRIBUTES| SNOW|SNOW_ATTRIBUTES| SNWD|SNWD_ATTRIBUTES|DAPR|DAPR_ATTRIBUTES|MDPR|MDPR_ATTRIBUTES|WESD|WESD_ATTRIBUTES|\n",
      "+-----------+----------+---------+-----------+---------+--------------------+-----+---------------+-----+---------------+-----+---------------+----+---------------+----+---------------+----+---------------+\n",
      "|CA1AB000001|2014-07-04|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|    0|            ,,N|    0|            ,,N|    0|            ,,N|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-05|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|  241|            ,,N|    0|            ,,N|    0|            ,,N|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-06|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|   38|            ,,N|    0|            ,,N|    0|            ,,N|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-07|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|    0|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-08|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|    0|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-09|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|    0|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-10|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|    5|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-11|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|    5|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-12|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|    0|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-13|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|    0|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-14|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|    0|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-15|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|    0|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-16|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|    0|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-17|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|    3|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-18|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|   94|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-19|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|   33|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-20|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|   18|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-21|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|   46|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-22|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|    0|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "|CA1AB000001|2014-07-23|53.606907|-113.561926|    686.7|EDMONTON 9.1 NNW,...|    0|            ,,N|    0|            ,,N| null|           null|null|           null|null|           null|null|           null|\n",
      "+-----------+----------+---------+-----------+---------+--------------------+-----+---------------+-----+---------------+-----+---------------+----+---------------+----+---------------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def daily_summaries_schema():\n",
    "    return types.StructType([\n",
    "        types.StructField(\"station\", types.StringType()),\n",
    "        types.StructField(\"date\", types.StringType()),\n",
    "        types.StructField(\"latitude\", types.StringType()),\n",
    "        types.StructField(\"longtitude\", types.StringType()),\n",
    "        types.StructField(\"elevation\", types.StringType()),\n",
    "        types.StructField(\"name\", types.StringType()),\n",
    "        types.StructField(\"prcp\", types.StringType()),\n",
    "        types.StructField(\"prcp_attributes\", types.StringType()),\n",
    "        types.StructField(\"snow\", types.StringType()),\n",
    "        types.StructField(\"snow_attributes\", types.StringType()),\n",
    "        types.StructField(\"snwd\", types.StringType()),\n",
    "        types.StructField(\"snwd_attributes\", types.StringType()),\n",
    "        types.StructField(\"dapr\", types.StringType()),\n",
    "        types.StructField(\"dapr_attributes\", types.StringType()),\n",
    "        types.StructField(\"mdpr\", types.StringType()),\n",
    "        types.StructField(\"mdpr_attributes\", types.StringType()),\n",
    "        types.StructField(\"wesd\", types.StringType()),\n",
    "        types.StructField(\"wesd_attributes\", types.StringType()),\n",
    "    ])\n",
    "# show the data for the station CA1AB000001\n",
    "daily_summaries_data = spark.read.csv(data_path + \"/ghcnd-daily-summaries-latest-canada/CA1AB000001.csv\", sep=\",\", header=True)\n",
    "daily_summaries_data.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GHCND-all\n",
    "See [documentation](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt)\n",
    "III. FORMAT OF DATA FILES (\".dly\" FILES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def ghcnd_all_schema():\n",
    "    return types.StructType([\n",
    "        types.StructField(\"station\", types.StringType()),\n",
    "        types.StructField(\"year\", types.StringType()),\n",
    "        types.StructField(\"month\", types.StringType()),\n",
    "        types.StructField(\"element\", types.StringType()),\n",
    "        types.StructField(\"value1\", types.StringType()),\n",
    "        types.StructField(\"mflag1\", types.StringType()),\n",
    "        types.StructField(\"qflag1\", types.StringType()),\n",
    "        types.StructField(\"sflag1\", types.StringType()),\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    splitted_data = line.split(\" \")\n",
    "    while \"\" in splitted_data:\n",
    "        splitted_data.remove(\"\")\n",
    "\n",
    "    station = splitted_data[0][:11]\n",
    "    year = splitted_data[0][11:15]\n",
    "    month = splitted_data[0][15:17]\n",
    "    element = splitted_data[0][17:21]\n",
    "    value1 = splitted_data[0][21:]\n",
    "    mflag1 = splitted_data[1]\n",
    "    qflag1 = splitted_data[2]\n",
    "    sflag1 = splitted_data[3]\n",
    "\n",
    "    return Row(station, year, month, element, value1, mflag1, qflag1, sflag1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----+-------+------+------+------+------+\n",
      "|    station|year|month|element|value1|mflag1|qflag1|sflag1|\n",
      "+-----------+----+-----+-------+------+------+------+------+\n",
      "|CA1AB000001|2014|   07|   PRCP| -9999| -9999| -9999|     0|\n",
      "|CA1AB000001|2014|   07|   SNOW| -9999| -9999| -9999|     0|\n",
      "|CA1AB000001|2014|   07|   SNWD| -9999| -9999| -9999|     0|\n",
      "|CA1AB000001|2014|   08|   PRCP|      |     0|     N|     0|\n",
      "|CA1AB000001|2014|   08|   SNOW|      |     0|     N|     0|\n",
      "|CA1AB000001|2014|   08|   SNWD| -9999| -9999| -9999| -9999|\n",
      "|CA1AB000001|2014|   08|   DAPR| -9999| -9999| -9999| -9999|\n",
      "|CA1AB000001|2014|   08|   MDPR| -9999| -9999| -9999| -9999|\n",
      "|CA1AB000001|2014|   08|   WESD| -9999| -9999| -9999| -9999|\n",
      "|CA1AB000001|2014|   09|   PRCP|      |    0T|     N|     5|\n",
      "|CA1AB000001|2014|   09|   SNOW|      |     0|     N|     0|\n",
      "|CA1AB000001|2014|   09|   SNWD| -9999| -9999| -9999| -9999|\n",
      "|CA1AB000001|2014|   09|   DAPR| -9999| -9999| -9999| -9999|\n",
      "|CA1AB000001|2014|   09|   MDPR| -9999| -9999| -9999| -9999|\n",
      "|CA1AB000001|2014|   10|   PRCP|      |     0|     N|     8|\n",
      "|CA1AB000001|2014|   10|   SNOW|      |     0|     N|     0|\n",
      "|CA1AB000001|2014|   10|   SNWD|      |     0|     N|     0|\n",
      "|CA1AB000001|2016|   03|   PRCP| -9999| -9999| -9999| -9999|\n",
      "|CA1AB000001|2016|   03|   SNOW| -9999| -9999| -9999| -9999|\n",
      "|CA1AB000001|2016|   03|   SNWD| -9999| -9999| -9999| -9999|\n",
      "+-----------+----+-----+-------+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ghcnd_all_input = sc.textFile(data_path + \"/ghcnd-all-canada/CA1AB000001.dly\")\n",
    "formatted_lines = ghcnd_all_input.map(parse_line)\n",
    "cleaned_ghcnd_all = spark.createDataFrame(data=formatted_lines, schema=ghcnd_all_schema())\n",
    "cleaned_ghcnd_all.show()\n",
    "# TODO more data ... can i split this better :("
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wind"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Temperature"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Weather"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2: Clean up the data\n",
    "## CSV to Parquet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}