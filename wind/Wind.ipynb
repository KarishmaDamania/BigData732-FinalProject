{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# How the data looks like\n",
    "```\n",
    "station,date,element,value,qflag,mflag,sflag,\n",
    "AE000041196,20210101,TMAX,278,,,S,\n",
    "AE000041196,20210101,PRCP,0,D,,S,\n",
    "AE000041196,20210101,TAVG,214,H,,S,\n",
    "AEM00041194,20210101,TMAX,266,,,S,\n",
    "AEM00041194,20210101,TMIN,178,,,S,\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession, functions, types, Row\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "## DataFrames\n",
    "spark = SparkSession.builder.appName('Canadian wind').getOrCreate()\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "assert spark.version >= '3.0'  # make sure we have Spark 3.0+\n",
    "## RDDs\n",
    "sc = spark.sparkContext\n",
    "assert sc.version >= '3.0'\n",
    "\n",
    "# General variables\n",
    "data_path = \"../data.nosync\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Group the data by states, because we want to find specific regions that are good for refugee settlement"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "stations = spark.read.parquet(data_path + \"/ghcnd-stations-cleaned\").withColumnRenamed(\"id\", \"station\")\n",
    "#stations.show()\n",
    "\n",
    "def get_data_schema():\n",
    "    return types.StructType([\n",
    "        types.StructField('station', types.StringType()),\n",
    "        types.StructField('date', types.StringType()),\n",
    "        types.StructField('observation', types.StringType()),\n",
    "        types.StructField('value', types.IntegerType()),\n",
    "        types.StructField('mflag', types.StringType()),\n",
    "        types.StructField('qflag', types.StringType()),\n",
    "        types.StructField('sflag', types.StringType()),\n",
    "        types.StructField('obstime', types.StringType()),\n",
    "    ])\n",
    "\n",
    "data = spark.read.csv(data_path + \"/cluster-data\", schema=get_data_schema())\n",
    "cleaned_data = data.where(data[\"station\"].startswith(\"CA\"))\n",
    "#cleaned_data.show()\n",
    "\n",
    "merged = stations.join(cleaned_data, \"station\").select(\"station\", \"state\", \"date\", \"observation\", \"value\").where(data[\"qflag\"] != \"null\")\n",
    "\n",
    "# only once:\n",
    "# merged.write.partitionBy(\"state\").parquet(data_path + \"/cluster-data-cleaned\", mode=\"overwrite\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Group the data by observations such that I can focus on specific elements and read the data optimally\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}